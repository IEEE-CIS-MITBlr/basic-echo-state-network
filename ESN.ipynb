{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic implementation of Echo State Network\n",
    "\n",
    "**Welcome!** 👋\n",
    "In this project, we aim to help you understand and build a basic version of an Echo state Network (ESN) and using it to forcast the future values of the *Mackey-Glass time series*, a well known chaotic system. Through this notebook, you will gain hands-on experience in applying a reservoir computing approach to a challenging time series prediction task.\n",
    "\n",
    "## What You Will Learn\n",
    "\n",
    "1. **Mackey-Glass Time Series:**  \n",
    "   We will introduce the Mackey-Glass series, describing its generation process and its characteristic chaotic behavior. You will also learn why it's frequently used as a benchmark for forecasting models.\n",
    "\n",
    "2. **Echo State Networks (ESNs):**  \n",
    "   You will get a conceptual overview of ESNs: what the reservoir is, how we control its dynamics through the spectral radius and input scaling, and why we can train the output layer with a simple linear regression approach rather than backpropagation.\n",
    "\n",
    "3. **Practical Steps to Implementation:**  \n",
    "   - How to generate and prepare the Mackey-Glass data (normalization, train-test split).  \n",
    "   - How to initialize and run the ESN.  \n",
    "   - How to train the readout weights using ridge regression.  \n",
    "   - How to visualize and evaluate the ESN's performance on unseen data.\n",
    "\n",
    "## Why ESNs?\n",
    "\n",
    "Echo State Networks offer a unique combination of simplicity and power. They avoid some complexities of traditional recurrent networks by fixing the recurrent weights and training only the output layer. This leads to:\n",
    "\n",
    "- **Efficient Training:** No need for complex backpropagation through time.  \n",
    "- **Rich Dynamics:** The reservoir's random, recurrent structure can model a wide range of temporal patterns.  \n",
    "- **Flexibility:** ESNs can be applied to many sequence-related problems with minimal adjustments.\n",
    "\n",
    "## The Goal\n",
    "\n",
    "By the end of this notebook, you will have a working ESN applied to a challenging nonlinear time series. Beyond just code execution, you will deepen your understanding of the underlying concepts, empowering you to experiment with different parameter settings and datasets.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on Echo State Networks\n",
    "\n",
    "### What is Reservoir Computing?\n",
    "\n",
    "Reservoir computing is a computational framework derived from several recurrent neural network models. It is a model suited for temporal or sequential data processing, like time-series prediction, speech recognition and system modelling. Generally, a reservoir computing system will consist of a reservoir for mapping inputs, and a readout for pattern analysis. Reservoir computing often uses randomly connected networks to capture complex dynamics.  \n",
    "A good reservoir system should be able to separate inputs. This is called separability condition.\n",
    "\n",
    "### Key Concepts in Echo State Networks (ESNs)\n",
    "\n",
    "1. **Reservoir:**  \n",
    "   The reservoir is a large set of interconnected neurons with fixed and non-trainable random weights. When you feed time series data into the reservoir, it produces complex, nonlinear responses over time. This internal state can be thought of as a dynamic memory that retains information about recent inputs.\n",
    "\n",
    "2. **Spectral Radius:**  \n",
    "   The spectral radius is the largest absolute value of the reservoir's weight matrix eigenvalues. It controls the reservoir's stability and how signals propagate and fade over time. Typically, it's set to a value slightly less than 1 to ensure that the echo states eventually die out and don't explode. To make sense of this, consider a body of water like a pond, when a pebble is dropped into the water it produces ripples that eventually get weaker and disappear. This is what we try to simulate\n",
    "\n",
    "3. **Input Weights:**  \n",
    "   The input weights determine how external inputs drive the reservoir. Scaling these appropriately ensures the reservoir receives a meaningful range of values, neither saturating nor ignoring the input.\n",
    "\n",
    "4. **Output (Readout) Weights:**  \n",
    "   After passing the input through the reservoir, we train a set of linear output weights to map the reservoir's state to the desired target output. Because this is a simple linear regression problem, it's easy and fast to solve without backpropagating through the entire network.\n",
    "\n",
    "### Why ESNs?\n",
    "\n",
    "ESNs are popular for time series forecasting because they simplify training. Since you only adjust the readout weights, you avoid complex backpropagation through time. This efficiency is especially appealing for long sequences or chaotic data. ESNs can capture rich temporal patterns thanks to their large, random reservoir, and they often perform well on challenging nonlinear forecasting tasks.\n",
    "\n",
    "### Key Parameters to Consider\n",
    "\n",
    "- **Reservoir Size:**  \n",
    "  The number of neurons in the reservoir. A larger reservoir might capture more complex dynamics but will be more computationally expensive.\n",
    "\n",
    "- **Spectral Radius:**  \n",
    "  Typically chosen slightly less than 1.0 to maintain the echo state property and stable dynamics.\n",
    "\n",
    "- **Leaking Rate:**  \n",
    "  Controls how quickly the reservoir's state is updated. A lower leaking rate slows down state changes, providing a longer “memory” of the past.\n",
    "\n",
    "- **Regularization:**  \n",
    "  When training the readout weights, adding ridge regression (L2 regularization) can prevent overfitting and improve generalization.\n",
    "\n",
    "## Mackey-Glass Time Series\n",
    "\n",
    "The Mackey-Glass time series comes from a mathematical equation that defines how certain quantities, like blood cells, change over time with a delay. Basically, it is able to model a system whose values depend on the current values as well as those that occurred a while ago. THis gives it a \"memory\" effect which makes it look chaotic even though it depends on a fixed equation\n",
    "\n",
    "**Why is it used as a benchmark?**  \n",
    "\n",
    "The Mackey-Glass series looks irregular and can be hard to predict which makes it an excellent test for evaluating forecasting methods. If a model can handle the Mackey-Glass series, it's likely capable of dealing with other challenging, real-world sequences.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Before we begin, let's make sure we have all the required dependencies installed. For this tutorial, we will be using:\n",
    "\n",
    "- **NumPy:** For numerical computations and handling arrays.\n",
    "- **Matplotlib:** For plotting the time series and visualization of predictions.\n",
    "\n",
    "If you are running this notebook locally and don’t have these packages, you can install them directly from within the notebook. If you are running on an environment like Google Colab or a pre-configured environment, these should already be available.\n",
    "\n",
    "Run the cell below if you need to install or upgrade any of these packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.24.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: matplotlib in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arnav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Prepare the Mackey-Glass Data\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. **Generate the Mackey-Glass time series:**  \n",
    "   We will use a function to approximate the Mackey-Glass differential equation and produce a series of values.  \n",
    "   \n",
    "2. **Normalize the data:**  \n",
    "   Normalization ensures that the input values are on a similar scale, which helps stabilize the training and improves the performance of the ESN.\n",
    "\n",
    "3. **Split into Training and Test Sets:**  \n",
    "   We will use the first portion of the data for training the ESN and the remaining portion for testing the model’s forecasting ability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 2100\n",
      "Test data length: 900\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_mackey_glass(beta=0.2, gamma=0.1, tau=17, n_samples=3000, delta_t=1.0, seed=42):\n",
    "    \"\"\"\n",
    "    Generate Mackey-Glass time series using a discrete approximation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta, gamma : float\n",
    "        Parameters of the Mackey-Glass equation.\n",
    "        beta controls how strongly the delayed input contributes to the system's growth.\n",
    "        gamma controls how quickly the current value of x decays or is lost over time\n",
    "    tau : int\n",
    "        Time delay paramter. How long it takes for the past state to influence the current state\n",
    "    n_samples : int\n",
    "        Length of the time series to generate.\n",
    "    delta_t : float\n",
    "        Time step increment for the approximation.\n",
    "    seed : int\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : np.ndarray\n",
    "        Generated Mackey-Glass time series of length n_samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    # We need a history array of length n_samples+tau to allow for delayed referencing\n",
    "    x = np.zeros(n_samples + tau)\n",
    "    # Initialize with some random values to start (you can also initialize with a constant)\n",
    "    # We do this because the equation needs to have some \"history\"\n",
    "    x[:tau] = 1.5 + 0.2 * np.random.randn(tau)\n",
    "    # Implementing of the equation\n",
    "    for t in range(tau, n_samples + tau - 1):\n",
    "        x_tau = x[t - tau]\n",
    "        dx = (beta * x_tau / (1 + x_tau**10)) - gamma * x[t]\n",
    "        x[t+1] = x[t] + dx * delta_t\n",
    "\n",
    "    # Return only the portion after the initial transient\n",
    "    return x[tau:]\n",
    "\n",
    "# Generate the Mackey-Glass series\n",
    "data = generate_mackey_glass(n_samples=3000)\n",
    "\n",
    "# Normalize the data\n",
    "data_mean = np.mean(data)\n",
    "data_std = np.std(data)\n",
    "data = (data - data_mean) / data_std\n",
    "\n",
    "# Split into train and test sets\n",
    "train_fraction = 0.7\n",
    "train_size = int(len(data)*train_fraction)\n",
    "train_data = data[:train_size]\n",
    "test_data = data[train_size:]\n",
    "\n",
    "print(\"Train data length:\", len(train_data))\n",
    "print(\"Test data length:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESN Implementation\n",
    "\n",
    "In this section, we will describe the key steps involved in setting up and using an Echo State Network:\n",
    "\n",
    "1. **Initializing the Reservoir and Weights:**\n",
    "   - We create a random recurrent weight matrix `W` for the reservoir and an input weight matrix `Win`.\n",
    "   - The reservoir’s hidden states are not trained directly, only the output layer weights will be learned.\n",
    "\n",
    "2. **Adjusting the Spectral Radius:**\n",
    "   - We rescale the reservoir weight matrix `W` so that its largest eigenvalue (spectral radius) is slightly less than 1.\n",
    "   - This ensures stability and the \"echo state property,\" meaning that the internal states will not explode or vanish over time. (remember the pond analogy)\n",
    "\n",
    "3. **Updating Reservoir States Given Input:**\n",
    "   - At each time step, we feed the current input into the reservoir.\n",
    "   - The reservoir state `x` is updated with a combination of the previous state, the new input, and the recurrent dynamics.\n",
    "   - A “leaking rate” can control how fast the reservoir state changes.\n",
    "\n",
    "4. **Collecting States for Training the Readout Layer:**\n",
    "   - As we run the reservoir on the training data, we record all reservoir states.\n",
    "   - We then use these recorded states (and the known target outputs) to solve a simple linear regression problem for the readout weights.\n",
    "\n",
    "5. **Using Ridge Regression to Compute Readout Weights:**\n",
    "   - Once we have a matrix of collected states and a vector of training targets, we use ridge regression (linear regression with L2 regularization) to find `Wout`.\n",
    "   - `Wout` is the set of weights from the reservoir states to the output, and it is the only part of the ESN that gets trained.\n",
    "\n",
    "Once the ESN is set up and `Wout` is computed, the network is ready to make predictions: it can generate outputs for new inputs by just updating the reservoir states and applying `Wout`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "# ESN Hyperparameters\n",
    "input_dim = 1\n",
    "reservoir_size = 200\n",
    "output_dim = 1\n",
    "spectral_radius = 0.95\n",
    "input_scaling = 0.1\n",
    "leaking_rate = 1.0\n",
    "ridge_reg = 1e-6  # Regularization parameter for ridge regression\n",
    "\n",
    "# Initialize input and reservoir weights\n",
    "Win = (np.random.rand(reservoir_size, input_dim) - 0.5) * 2 * input_scaling\n",
    "W = np.random.rand(reservoir_size, reservoir_size) - 0.5\n",
    "\n",
    "# Adjust spectral radius\n",
    "rhoW = max(abs(np.linalg.eigvals(W)))\n",
    "W *= (spectral_radius / rhoW)\n",
    "\n",
    "# At this point, the ESN is initialized. There will be no output for this cell\n",
    "# We haven't trained Wout yet, that will happen after we collect reservoir states on the training data. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
